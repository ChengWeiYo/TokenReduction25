
@article{touvron_training_2021,
	title = {Training data-efficient image transformers \& distillation through attention},
	abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
	urldate = {2021-05-11},
	journal = {arXiv:2012.12877 [cs]},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},
	month = jan,
	year = {2021},
	note = {arXiv: 2012.12877},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\MN3MX9L4\\Touvron et al. - 2021 - Training data-efficient image transformers & disti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\QH92I9AL\\2012.html:text/html},
}

@inproceedings{rios_anime_2022,
	title = {Anime {Character} {Recognition} using {Intermediate} {Features} {Aggregation}},
	copyright = {All rights reserved},
	doi = {10.1109/ISCAS48785.2022.9937519},
	abstract = {In this work we study the problem of anime character recognition. Anime, refers to animation produced within Japan and work derived or inspired from it. We propose a novel Intermediate Features Aggregation classification head, which helps smooth the optimization landscape of Vision Transformers (ViTs) by adding skip connections between intermediate layers and the classification head, thereby improving relative classification accuracy by up to 28\%. The proposed model, named as Animesion, is the first end-to-end framework for large-scale anime character recognition. We conduct extensive experiments using a variety of classification models, including CNNs and self-attention based ViTs. We also adapt its multimodal variation Vision-Language Transformer (ViLT), to incorporate external tag data for classification, without additional multimodal pre-training. Through our results we obtain new insights into the effects of how hyperparameters such as input sequence length, mini-batch size, and variations on the architecture, affect the transfer learning performance of Vi(L)Ts.},
	booktitle = {2022 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	author = {Rios, Edwin Arkel and Hu, Min-Chun and Lai, Bo-Cheng},
	month = may,
	year = {2022},
	note = {ISSN: 2158-1525},
	keywords = {Computer architecture, Transfer learning, Animation, Adaptation models, Circuits and systems, Sensitivity, Transformers},
	pages = {424--428},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\ZYRQAKEG\\9937519.html:text/html},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-08-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\BCYL2N7V\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@article{wei_fine-grained_2021,
	title = {Fine-{Grained} {Image} {Analysis} with {Deep} {Learning}: {A} {Survey}},
	issn = {1939-3539},
	shorttitle = {Fine-{Grained} {Image} {Analysis} with {Deep} {Learning}},
	doi = {10.1109/TPAMI.2021.3126648},
	abstract = {Fine-grained image analysis (FGIA) is a longstanding and fundamental problem in computer vision and pattern recognition, which underpins a diverse set of real-world applications. The task of FGIA targets analyzing visual objects from subordinate categories, e.g., species of birds or models of cars. The small inter-class and large intra-class variation inherent to fine-grained image analysis makes it a challenging problem. Capitalizing on advances in deep learning, in recent years we have witnessed remarkable progress in deep learning powered FGIA. In this paper we present a systematic survey of these advances, where we attempt to re-define and broaden the field of FGIA by consolidating two fundamental fine-grained research areas – fine-grained image recognition and fine-grained image retrieval. In addition, we also review other key issues of FGIA, such as publicly available benchmark datasets and related domain-specific applications. We conclude by highlighting several research directions and open problems which need further exploration from the community.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wei, Xiu-Shen and Song, Yi-Zhe and Mac Aodha, Oisin and Wu, Jianxin and Peng, Yuxin and Tang, Jinhui and Yang, Jian and Belongie, Serge},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Deep learning, Task analysis, Image analysis, Deep Learning, Image retrieval, Visualization, Birds, Image recognition, Fine-Grained Image Recognition, Fine-Grained Image Retrieval, Fine-Grained Images Analysis},
	pages = {1--1},
	file = {Accepted Version:C\:\\Users\\edwin\\Zotero\\storage\\RTZ25B5F\\Wei et al. - 2021 - Fine-Grained Image Analysis with Deep Learning A .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\2FQD5LBI\\9609630.html:text/html},
}

@inproceedings{huang_deep_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep {Networks} with {Stochastic} {Depth}},
	isbn = {978-3-319-46493-0},
	doi = {10.1007/978-3-319-46493-0_39},
	abstract = {Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91 \% on CIFAR-10).},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Constant Depth, Early Layer, Test Error, Training Time, Validation Error},
	pages = {646--661},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\R57KEP43\\Huang et al. - 2016 - Deep Networks with Stochastic Depth.pdf:application/pdf},
}

@incollection{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8024--8035},
}

@misc{biewald_experiment_2020,
	title = {Experiment {Tracking} with {Weights} and {Biases}},
	author = {Biewald, Lukas},
	year = {2020},
	annote = {Software available from wandb.com},
}

@inproceedings{hu_rams-trans_2021,
	address = {New York, NY, USA},
	series = {{MM} '21},
	title = {{RAMS}-{Trans}: {Recurrent} {Attention} {Multi}-scale {Transformer} for {Fine}-grained {Image} {Recognition}},
	isbn = {978-1-4503-8651-7},
	shorttitle = {{RAMS}-{Trans}},
	doi = {10.1145/3474085.3475561},
	abstract = {In fine-grained image recognition (FGIR), the localization and amplification of region attention is an important factor, which has been explored extensively convolutional neural networks (CNNs) based approaches. The recently developed vision transformer (ViT) has achieved promising results in computer vision tasks. Compared with CNNs, Image sequentialization is a brand new manner. However, ViT is limited in its receptive field size and thus lacks local attention like CNNs due to the fixed size of its patches, and is unable to generate multi-scale features to learn discriminative region attention. To facilitate the learning of discriminative region attention without box/part annotations, we use the strength of the attention weights to measure the importance of the patch tokens corresponding to the raw images. We propose the recurrent attention multi-scale transformer (RAMS-Trans), which uses the transformer's self-attention to recursively learn discriminative region attention in a multi-scale manner. Specifically, at the core of our approach lies the dynamic patch proposal module (DPPM) responsible for guiding region amplification to complete the integration of multi-scale image patches. The DPPM starts with the full-size image patches and iteratively scales up the region attention to generate new patches from global to local by the intensity of the attention weights generated at each scale as an indicator. Our approach requires only the attention weights that come with ViT itself and can be easily trained end-to-end. Extensive experiments demonstrate that RAMS-Trans performs better than exising works, in addition to efficient CNN models, achieving state-of-the-art results on three benchmark datasets.},
	urldate = {2022-08-23},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Hu, Yunqing and Jin, Xuan and Zhang, Yin and Hong, Haiwen and Zhang, Jingfeng and He, Yuan and Xue, Hui},
	month = oct,
	year = {2021},
	keywords = {transformer, fine-grained image recognition, multi-scale, region attention},
	pages = {4239--4248},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\4GT3HZWD\\Hu et al. - 2021 - RAMS-Trans Recurrent Attention Multi-scale Transf.pdf:application/pdf},
}

@misc{hu_see_2019,
	title = {See {Better} {Before} {Looking} {Closer}: {Weakly} {Supervised} {Data} {Augmentation} {Network} for {Fine}-{Grained} {Visual} {Classification}},
	shorttitle = {See {Better} {Before} {Looking} {Closer}},
	doi = {10.48550/arXiv.1901.09891},
	abstract = {Data augmentation is usually adopted to increase the amount of training data, prevent overfitting and improve the performance of deep models. However, in practice, random data augmentation, such as random image cropping, is low-efficiency and might introduce many uncontrolled background noises. In this paper, we propose Weakly Supervised Data Augmentation Network (WS-DAN) to explore the potential of data augmentation. Specifically, for each training image, we first generate attention maps to represent the object's discriminative parts by weakly supervised learning. Next, we augment the image guided by these attention maps, including attention cropping and attention dropping. The proposed WS-DAN improves the classification accuracy in two folds. In the first stage, images can be seen better since more discriminative parts' features will be extracted. In the second stage, attention regions provide accurate location of object, which ensures our model to look at the object closer and further improve the performance. Comprehensive experiments in common fine-grained visual classification datasets show that our WS-DAN surpasses the state-of-the-art methods, which demonstrates its effectiveness.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {Hu, Tao and Qi, Honggang and Huang, Qingming and Lu, Yan},
	month = mar,
	year = {2019},
	note = {arXiv:1901.09891 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\BR6MDBEB\\Hu et al. - 2019 - See Better Before Looking Closer Weakly Supervise.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\QRF9LVD6\\1901.html:text/html},
}

@inproceedings{gwilliam_fair_2021,
	title = {Fair {Comparison}: {Quantifying} {Variance} in {Results} for {Fine}-{Grained} {Visual} {Categorization}},
	shorttitle = {Fair {Comparison}},
	language = {en},
	urldate = {2022-08-23},
	author = {Gwilliam, Matthew and Teuscher, Adam and Anderson, Connor and Farrell, Ryan},
	year = {2021},
	pages = {3309--3318},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\DYYAHUU8\\Gwilliam et al. - 2021 - Fair Comparison Quantifying Variance in Results f.pdf:application/pdf;Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\3V8F5A66\\Gwilliam_Fair_Comparison_Quantifying_Variance_in_Results_for_Fine-Grained_Visual_Categorization.html:text/html},
}

@inproceedings{zhang_part-based_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Part-{Based} {R}-{CNNs} for {Fine}-{Grained} {Category} {Detection}},
	isbn = {978-3-319-10590-1},
	doi = {10.1007/978-3-319-10590-1_54},
	abstract = {Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Zhang, Ning and Donahue, Jeff and Girshick, Ross and Darrell, Trevor},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {object detection, convolutional models, Fine-grained recognition},
	pages = {834--849},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\XYUGFKUC\\Zhang et al. - 2014 - Part-Based R-CNNs for Fine-Grained Category Detect.pdf:application/pdf},
}

@article{wei_mask-cnn_2018,
	title = {Mask-{CNN}: {Localizing} parts and selecting descriptors for fine-grained bird species categorization},
	volume = {76},
	issn = {0031-3203},
	shorttitle = {Mask-{CNN}},
	doi = {10.1016/j.patcog.2017.10.002},
	abstract = {Fine-grained image recognition is a challenging computer vision problem, due to the small inter-class variations caused by highly similar subordinate categories, and the large intra-class variations in poses, scales and rotations. In this paper, we prove that selecting useful deep descriptors contributes well to fine-grained image recognition. Specifically, a novel Mask-CNN model without the fully connected layers is proposed. Based on the part annotations, the proposed model consists of a fully convolutional network to both locate the discriminative parts (e.g., head and torso), and more importantly generate weighted object/part masks for selecting useful and meaningful convolutional descriptors. After that, a three-stream Mask-CNN model is built for aggregating the selected object- and part-level descriptors simultaneously. Thanks to discarding the parameter redundant fully connected layers, our Mask-CNN has a small feature dimensionality and efficient inference speed by comparing with other fine-grained approaches. Furthermore, we obtain a new state-of-the-art accuracy on two challenging fine-grained bird species categorization datasets, which validates the effectiveness of both the descriptor selection scheme and the proposed Mask-CNN model.},
	language = {en},
	urldate = {2022-08-23},
	journal = {Pattern Recognition},
	author = {Wei, Xiu-Shen and Xie, Chen-Wei and Wu, Jianxin and Shen, Chunhua},
	month = apr,
	year = {2018},
	keywords = {Fine-grained image recognition, Deep descriptor selection, Part localization},
	pages = {704--714},
	file = {ScienceDirect Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\UWB69H68\\S0031320317303990.html:text/html},
}

@article{dosovitskiy_image_2020,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2020-12-14},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\7XLVFF8L\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\WMXE926K\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\VQ2D9SJR\\Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\TEA5C6AT\\Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\4TBHJTHS\\2010.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\L46UW9JK\\2010.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\5H3FWVBB\\2010.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\C8VA2DFU\\2010.html:text/html},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-01-12},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\S8G9YEBZ\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\2V8SGB3J\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\AKNTY5T5\\1810.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\FY49HZR2\\1810.html:text/html},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2020-11-22},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	annote = {Comment: Tech report},
	annote = {Comment: Tech report},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\ERIG3UGW\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\TLIL75XD\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\GQXASNS8\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\D9GJSWY8\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\JJX2GDIP\\1512.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\DUY63VKJ\\1512.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\QUUN6JQZ\\1512.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\NUHLZPKZ\\1512.html:text/html},
}

@inproceedings{wang_feature_2021,
	title = {Feature {Fusion} {Vision} {Transformer} for {Fine}-{Grained} {Visual} {Categorization}},
	abstract = {The core for tackling the fine-grained visual categorization (FGVC) is to learn subtle yet discriminative features. Most previous works achieve this by explicitly selecting the discriminative parts or integrating the attention mechanism via CNN-based approaches.However, these methods enhance the computational complexity and make the modeldominated by the regions containing the most of the objects. Recently, vision trans-former (ViT) has achieved SOTA performance on general image recognition tasks. Theself-attention mechanism aggregates and weights the information from all patches to the classification token, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation token in the deep layer pays more attention to the global information, lacking the local and low-level features that are essential for FGVC. In this work, we proposea novel pure transformer-based framework Feature Fusion Vision Transformer (FFVT)where we aggregate the important tokens from each transformer layer to compensate thelocal, low-level and middle-level information. We design a novel token selection mod-ule called mutual attention weight selection (MAWS) to guide the network effectively and efficiently towards selecting discriminative tokens without introducing extra param-eters. We verify the effectiveness of FFVT on three benchmarks where FFVT achieves the state-of-the-art performance.},
	urldate = {2021-12-29},
	booktitle = {British {Machine} {Vision} {Conference} ({BMVC})},
	author = {Wang, Jun and Yu, Xiaohan and Gao, Yongsheng},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.02341},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 9 pages, 2 figures, 3 tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\6WL43BUX\\Wang et al. - 2021 - Feature Fusion Vision Transformer for Fine-Grained.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\45PXFJLX\\2107.html:text/html},
}

@article{maji_fine-grained_2013,
	title = {Fine-{Grained} {Visual} {Classification} of {Aircraft}},
	abstract = {This paper introduces FGVC-Aircraft, a new dataset containing 10,000 images of aircraft spanning 100 aircraft models, organised in a three-level hierarchy. At the finer level, differences between models are often subtle but always visually measurable, making visual recognition challenging but possible. A benchmark is obtained by defining corresponding classification tasks and evaluation protocols, and baseline results are presented. The construction of this dataset was made possible by the work of aircraft enthusiasts, a strategy that can extend to the study of number of other object classes. Compared to the domains usually considered in fine-grained visual classification (FGVC), for example animals, aircraft are rigid and hence less deformable. They, however, present other interesting modes of variation, including purpose, size, designation, structure, historical style, and branding.},
	urldate = {2021-12-29},
	journal = {arXiv:1306.5151 [cs]},
	author = {Maji, Subhransu and Rahtu, Esa and Kannala, Juho and Blaschko, Matthew and Vedaldi, Andrea},
	month = jun,
	year = {2013},
	note = {arXiv: 1306.5151},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\8J9SD59M\\Maji et al. - 2013 - Fine-Grained Visual Classification of Aircraft.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\3J7IUMY2\\1306.html:text/html},
}

@article{wah_caltech-ucsd_nodate,
	title = {The {Caltech}-{UCSD} {Birds}-200-2011 {Dataset}},
	abstract = {CUB-200-2011 is an extended version of CUB-200 [7], a challenging dataset of 200 bird species. The extended version roughly doubles the number of images per category and adds new part localization annotations. All images are annotated with bounding boxes, part locations, and attribute labels. Images and annotations were ﬁltered by multiple users of Mechanical Turk. We introduce benchmarks and baseline experiments for multi-class categorization and part localization.},
	language = {en},
	author = {Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge},
	pages = {8},
	file = {Wah et al. - The Caltech-UCSD Birds-200-2011 Dataset.pdf:C\:\\Users\\edwin\\Zotero\\storage\\IJ4U82PD\\Wah et al. - The Caltech-UCSD Birds-200-2011 Dataset.pdf:application/pdf},
}

@misc{ba_layer_2016,
	title = {Layer {Normalization}},
	doi = {10.48550/arXiv.1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv:1607.06450 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\8LP22X5W\\Ba et al. - 2016 - Layer Normalization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\U2QCRQWS\\1607.html:text/html},
}

@inproceedings{he_transfg_2022,
	title = {{TransFG}: {A} {Transformer} {Architecture} for {Fine}-{Grained} {Recognition}},
	shorttitle = {{TransFG}},
	abstract = {Fine-grained visual classification (FGVC) which aims at recognizing objects from subcategories is a very challenging task due to the inherently subtle inter-class differences. Most existing works mainly tackle this problem by reusing the backbone network to extract features of detected discriminative regions. However, this strategy inevitably complicates the pipeline and pushes the proposed regions to contain most parts of the objects thus fails to locate the really important parts. Recently, vision transformer (ViT) shows its strong performance in the traditional classification task. The self-attention mechanism of the transformer links every patch token to the classification token. In this work, we first evaluate the effectiveness of the ViT framework in the fine-grained recognition setting. Then motivated by the strength of the attention link can be intuitively considered as an indicator of the importance of tokens, we further propose a novel Part Selection Module that can be applied to most of the transformer architectures where we integrate all raw attention weights of the transformer into an attention map for guiding the network to effectively and accurately select discriminative image patches and compute their relations. A contrastive loss is applied to enlarge the distance between feature representations of confusing classes. We name the augmented transformer-based model TransFG and demonstrate the value of it by conducting experiments on five popular fine-grained benchmarks where we achieve state-of-the-art performance. Qualitative results are presented for better understanding of our model.},
	language = {en},
	urldate = {2023-02-26},
	booktitle = {Proceedings of the {First} {MiniCon} {Conference}},
	author = {He, Ju and Chen, Jie-Neng and Liu, Shuai and Kortylewski, Adam and Yang, Cheng and Bai, Yutong and Wang, Changhu},
	month = feb,
	year = {2022},
}

@misc{abnar_quantifying_2020,
	title = {Quantifying {Attention} {Flow} in {Transformers}},
	doi = {10.48550/arXiv.2005.00928},
	abstract = {In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
	urldate = {2023-02-26},
	publisher = {arXiv},
	author = {Abnar, Samira and Zuidema, Willem},
	month = may,
	year = {2020},
	note = {arXiv:2005.00928 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\75B7RA2R\\Abnar and Zuidema - 2020 - Quantifying Attention Flow in Transformers.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\NQXMCWIK\\2005.html:text/html},
}

@inproceedings{hou_vegfru_2017,
	title = {{VegFru}: {A} {Domain}-{Specific} {Dataset} for {Fine}-{Grained} {Visual} {Categorization}},
	shorttitle = {{VegFru}},
	urldate = {2023-03-02},
	author = {Hou, Saihui and Feng, Yushan and Wang, Zilei},
	year = {2017},
	pages = {541--549},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\NASLYYXD\\Hou et al. - 2017 - VegFru A Domain-Specific Dataset for Fine-Grained.pdf:application/pdf},
}

@misc{van_horn_inaturalist_2018,
	title = {The {iNaturalist} {Species} {Classification} and {Detection} {Dataset}},
	doi = {10.48550/arXiv.1707.06642},
	abstract = {Existing image classification datasets used in computer vision tend to have a uniform distribution of images across object categories. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier to photograph than others. To encourage further progress in challenging real world conditions we present the iNaturalist species classification and detection dataset, consisting of 859,000 images from over 5,000 different species of plants and animals. It features visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have been verified by multiple citizen scientists. We discuss the collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classification and detection models. Results show that current non-ensemble based methods achieve only 67\% top one classification accuracy, illustrating the difficulty of the dataset. Specifically, we observe poor results for classes with small numbers of training examples suggesting more attention is needed in low-shot learning.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
	month = apr,
	year = {2018},
	note = {arXiv:1707.06642 [cs]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\NK867V77\\Van Horn et al. - 2018 - The iNaturalist Species Classification and Detecti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\KCFBYY37\\1707.html:text/html},
}

@inproceedings{houlsby_parameter-efficient_2019,
	title = {Parameter-{Efficient} {Transfer} {Learning} for {NLP}},
	abstract = {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to \$26\$ diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within \$0.8\%\$ of the performance of full fine-tuning, adding only \$3.6\%\$ parameters per task. By contrast, fine-tuning trains \$100\%\$ of the parameters per task.},
	language = {en},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and Laroussilhe, Quentin De and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2790--2799},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\P8BUUF6L\\Houlsby et al. - 2019 - Parameter-Efficient Transfer Learning for NLP.pdf:application/pdf;Supplementary PDF:C\:\\Users\\edwin\\Zotero\\storage\\BFAAVS4E\\Houlsby et al. - 2019 - Parameter-Efficient Transfer Learning for NLP.pdf:application/pdf},
}

@inproceedings{sun_sim-trans_2022,
	address = {New York, NY, USA},
	series = {{MM} '22},
	title = {{SIM}-{Trans}: {Structure} {Information} {Modeling} {Transformer} for {Fine}-grained {Visual} {Categorization}},
	isbn = {978-1-4503-9203-7},
	shorttitle = {{SIM}-{Trans}},
	doi = {10.1145/3503161.3548308},
	abstract = {Fine-grained visual categorization (FGVC) aims at recognizing objects from similar subordinate categories, which is challenging and practical for human's accurate automatic recognition needs. Most FGVC approaches focus on the attention mechanism research for discriminative regions mining while neglecting their interdependencies and composed holistic object structure, which are essential for model's discriminative information localization and understanding ability. To address the above limitations, we propose the Structure Information Modeling Transformer (SIM-Trans) to incorporate object structure information into transformer for enhancing discriminative representation learning to contain both the appearance information and structure information. Specifically, we encode the image into a sequence of patch tokens and build a strong vision transformer framework with two well-designed modules: (i) the structure information learning (SIL) module is proposed to mine the spatial context relation of significant patches within the object extent with the help of the transformer's self-attention weights, which is further injected into the model for importing structure information; (ii) the multi-level feature boosting (MFB) module is introduced to exploit the complementary of multi-level features and contrastive learning among classes to enhance feature robustness for accurate fine-grained visual categorization. The proposed two modules are light-weighted and can be plugged into any transformer network and trained end-to-end easily, which only depends on the attention weights that come with the vision transformer itself. Extensive experiments and analyses demonstrate that the proposed SIM-Trans achieves state-of-the-art performance on fine-grained visual categorization benchmarks. The code will be available at https://github.com/PKU-ICST-MIPL/SIM-Trans\_ACMMM2022.},
	urldate = {2024-07-14},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Hongbo and He, Xiangteng and Peng, Yuxin},
	month = oct,
	year = {2022},
	pages = {5853--5861},
	file = {Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\598RHGU5\\Sun et al. - 2022 - SIM-Trans Structure Information Modeling Transfor.pdf:application/pdf},
}

@inproceedings{darcet_vision_2023,
	title = {Vision {Transformers} {Need} {Registers}},
	abstract = {Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.},
	language = {en},
	urldate = {2024-07-14},
	author = {Darcet, Timothée and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
	month = oct,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\VVDRBBKU\\Darcet et al. - 2023 - Vision Transformers Need Registers.pdf:application/pdf},
}

@article{guo_fine-grained_2023,
	title = {Fine-{Grained} {Ship} {Detection} in {High}-{Resolution} {Satellite} {Images} {With} {Shape}-{Aware} {Feature} {Learning}},
	volume = {16},
	issn = {2151-1535},
	doi = {10.1109/JSTARS.2023.3241969},
	abstract = {Fine-grained ship detection is an important task in high-resolution satellite remote sensing applications. However, large aspect ratios and severe category imbalance make fine-grained ship detection a challenging problem. Current methods usually extract square-like features that do not work well to detect ships with large aspect ratios, and the misalignments in feature representation will severely degrade the performance of ship localization and classification. To tackle this, we propose a shape-aware feature learning method to mitigate the misalignments during feature extraction. Furthermore, for the issue of category imbalance, we design a shape-aware instance switching to balance the quantity distribution of ships in different categories, which can greatly improve the network's learning ability for rare instances. To verify the effectiveness of the proposed method, we contribute a multicategory ship detection dataset (MCSD) that contains 4000 images carefully labeled with oriented bounding boxes, including 16 types of ship objects and nearly 18 000 instances. We conduct experiments on our MCSD and ShipRSImageNet, and extensive experimental results demonstrate the superiority of the proposed method over several state-of-the-art methods.},
	urldate = {2024-07-15},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Guo, Bo and Zhang, Ruixiang and Guo, Haowen and Yang, Wen and Yu, Huai and Zhang, Peng and Zou, Tongyuan},
	year = {2023},
	note = {Conference Name: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	keywords = {Task analysis, Feature extraction, Object detection, Detectors, Convolutional network, feature learning, Marine vehicles, oriented ship detection, Remote sensing, satellite image, Satellites},
	pages = {1914--1926},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\D2KY72BH\\10035980.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\LMSX55RK\\Guo et al. - 2023 - Fine-Grained Ship Detection in High-Resolution Sat.pdf:application/pdf},
}

@article{wang_efficient_2023,
	title = {Efficient {Fine}-{Grained} {Object} {Recognition} in {High}-{Resolution} {Remote} {Sensing} {Images} {From} {Knowledge} {Distillation} to {Filter} {Grafting}},
	volume = {61},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2023.3260883},
	abstract = {With the development of high-resolution remote sensing images (HR-RSIs) and the escalating demand for intelligent analysis, fine-grained recognition of geospatial objects has become a more practical and challenging task. Although deep learning-based object recognition has achieved superior performance, it is inflexible to be directly utilized to the fine-grained object recognition (FGOR) tasks of HR-RSIs under the limitation of the size of geospatial objects. An efficient fine-grained object recognition method in HR-RSIs from knowledge distillation (KL) to filter grafting is proposed. Specifically, fine-grained object recognition consists of two stages: Stage 1 utilizes oriented region convolutional neural network (oriented R-CNN) to accurately locate and preliminarily classify geospatial objects. At the same time, it serves as a teacher network to guide students’ effective learning of fine-grained object recognition; in Stage 2, we design a coarse-to-fine object recognition network (CF-ORNet), as the second teacher network, which realizes fine-grained recognition through feature learning and category correction. After that, we propose a lightweight model from knowledge distillation to filter grafting on two teacher networks to achieve efficient fine-grained object recognition. The experimental results on Vehicle Detection in Aerial Imagery (VEDAI) and HR Ship Collection 2016 (HRSC2016) datasets achieve competitive performance.},
	urldate = {2024-07-15},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Wang, Liuqian and Zhang, Jing and Tian, Jimiao and Li, Jiafeng and Zhuo, Li and Tian, Qi},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Computational modeling, Feature extraction, Image recognition, Knowledge engineering, Detectors, Object recognition, Coarse-to-fine object recognition network (CF-ORNet), filter grafting, fine-grained object recognition (FGOR), Geospatial analysis, high-resolution remote sensing image (HR-RSI), knowledge distillation},
	pages = {1--16},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\PTXF2GHC\\10078911.html:text/html},
}

@article{sun_fair1m_2022,
	title = {{FAIR1M}: {A} benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery},
	volume = {184},
	issn = {0924-2716},
	shorttitle = {{FAIR1M}},
	doi = {10.1016/j.isprsjprs.2021.12.004},
	abstract = {With the rapid development of deep learning, many deep learning-based approaches have made great achievements in object detection tasks. It is generally known that deep learning is a data-driven approach. Data directly impact the performance of object detectors to some extent. Although existing datasets include common objects in remote sensing images, they still have some scale, category, and image limitations. Therefore, there is a strong requirement for establishing a large-scale object detection benchmark for high-resolution remote sensing images. In this paper, we propose a novel benchmark dataset with more than 1 million instances and more than 40,000 images for Fine-grAined object recognItion in high-Resolution remote sensing imagery which is named as FAIR1M. We collected remote sensing images with a resolution of 0.3 m to 0.8 m from different platforms, which are spread across many countries and regions. All objects in the FAIR1M dataset are annotated with respect to 5 categories and 37 subcategories by oriented bounding boxes. Compared with existing detection datasets that are dedicated to object detection, the FAIR1M dataset has 4 particular characteristics: (1) it is much larger than other existing object detection datasets both in terms of the number of instances and the number of images, (2) it provides richer fine-grained category information for objects in remote sensing images, (3) it contains geographic information such as latitude, longitude and resolution attributes, and (4) it provides better image quality due to the use of a careful data cleaning procedure. Based on the FAIR1M dataset, we propose three fine-grained object detection and recognition tasks. Moreover, we evaluate several state-of-the-art approaches to establish baselines for future research. Experimental results indicate that the FAIR1M dataset effectively represents real remote sensing applications and is quite challenging for existing methods. Considering the fine-grained characteristics, we improve the evaluation metric and introduce the idea of hierarchy detection into the algorithms. We believe that the FAIR1M dataset will contribute to the earth observation community via fine-grained object detection in large-scale real-world scenes. FAIR1M Website: http://gaofen-challenge.com/.},
	urldate = {2024-07-15},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Sun, Xian and Wang, Peijin and Yan, Zhiyuan and Xu, Feng and Wang, Ruiping and Diao, Wenhui and Chen, Jin and Li, Jihao and Feng, Yingchao and Xu, Tao and Weinmann, Martin and Hinz, Stefan and Wang, Cheng and Fu, Kun},
	month = feb,
	year = {2022},
	keywords = {Deep learning, Benchmark dataset, Convolutional neural network (CNN), Fine-grained object detection and recognition, Remote sensing images},
	pages = {116--130},
	file = {ScienceDirect Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\HEM5Q5XY\\S0924271621003269.html:text/html;Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\X8T43U2F\\Sun et al. - 2022 - FAIR1M A benchmark dataset for fine-grained objec.pdf:application/pdf},
}

@article{ye_image_2024,
	title = {The {Image} {Data} and {Backbone} in {Weakly} {Supervised} {Fine}-{Grained} {Visual} {Categorization}: {A} {Revisit} and {Further} {Thinking}},
	volume = {34},
	issn = {1558-2205},
	shorttitle = {The {Image} {Data} and {Backbone} in {Weakly} {Supervised} {Fine}-{Grained} {Visual} {Categorization}},
	doi = {10.1109/TCSVT.2023.3284405},
	abstract = {Weakly-supervised fine-grained visual categorization (FGVC) aims to achieve subclass classification within the same large class using only label information. Compared to general images, fine-grained images have similar appearances and features, and are often affected by disturbances such as viewpoint, lighting, and occlusion during data collection, resulting in significant intra-class variance and small inter-class variance. To achieve FGVC, carefully designed models are often needed to explore the locally discriminative regions of the image. This paper revisits high-quality FGVC publications based on deep learning and analyzes from two new perspective: fine-grained image data and backbone. We address two ignored but interesting problems in FGVC. First, we argue that the reasons for exacerbating intra-class variance are not the same in data of animal, plant, and commodity types, and it is necessary to consider the effects of posture, covariate shift, and structural changes. Additionally, the “soft boundary” between subclasses intensifies the difficulty of classification. Second, we highlight that convolutional networks and self-attention networks have different receptive fields and shape biases, leading to performance differences when processing different types of fine-grained data. Overall, our analysis provides new insights into recent advances, challenges, and future directions for FGVC based on deep learning, which can help researchers develop more effective models for FGVC.},
	number = {1},
	urldate = {2024-07-15},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Ye, Shuo and Wang, Yu and Peng, Qinmu and You, Xinge and Chen, C. L. Philip},
	month = jan,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Data models, deep learning, Training, Visualization, Birds, Annotations, Automobiles, Fine-grained visual categorization, Analytical models, weakly supervised learning},
	pages = {2--16},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\H6GAZHZT\\10147290.html:text/html},
}

@misc{touvron_deit_2022,
	title = {{DeiT} {III}: {Revenge} of the {ViT}},
	shorttitle = {{DeiT} {III}},
	doi = {10.48550/arXiv.2204.07118},
	abstract = {A Vision Transformer (ViT) is a simple neural architecture amenable to serve several computer vision tasks. It has limited built-in architectural priors, in contrast to more recent architectures that incorporate priors either about the input data or of specific tasks. Recent works show that ViTs benefit from self-supervised pre-training, in particular BerT-like pre-training like BeiT. In this paper, we revisit the supervised training of ViTs. Our procedure builds upon and simplifies a recipe introduced for training ResNet-50. It includes a new simple data-augmentation procedure with only 3 augmentations, closer to the practice in self-supervised learning. Our evaluations on Image classification (ImageNet-1k with and without pre-training on ImageNet-21k), transfer learning and semantic segmentation show that our procedure outperforms by a large margin previous fully supervised training recipes for ViT. It also reveals that the performance of our ViT trained with supervision is comparable to that of more recent architectures. Our results could serve as better baselines for recent self-supervised approaches demonstrated on ViT.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Touvron, Hugo and Cord, Matthieu and Jégou, Hervé},
	month = apr,
	year = {2022},
	note = {arXiv:2204.07118 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\22T7KFRK\\Touvron et al. - 2022 - DeiT III Revenge of the ViT.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\4R8IEV65\\2204.html:text/html},
}

@misc{rios_global-local_2024,
	title = {Global-{Local} {Similarity} for {Efficient} {Fine}-{Grained} {Image} {Recognition} with {Vision} {Transformers}},
	copyright = {All rights reserved},
	abstract = {Fine-grained recognition involves the classification of images from subordinate macro-categories, and it is challenging due to small inter-class differences. To overcome this, most methods perform discriminative feature selection enabled by a feature extraction backbone followed by a high-level feature refinement step. Recently, many studies have shown the potential behind vision transformers as a backbone for fine-grained recognition, but their usage of its attention mechanism to select discriminative tokens can be computationally expensive. In this work, we propose a novel and computationally inexpensive metric to identify discriminative regions in an image. We compare the similarity between the global representation of an image given by the CLS token, a learnable token used by transformers for classification, and the local representation of individual patches. We select the regions with the highest similarity to obtain crops, which are forwarded through the same transformer encoder. Finally, high-level features of the original and cropped representations are further refined together in order to make more robust predictions. Through extensive experimental evaluation we demonstrate the effectiveness of our proposed method, obtaining favorable results in terms of accuracy across a variety of datasets. Furthermore, our method achieves these results at a much lower computational cost compared to the alternatives. Code and checkpoints are available at: {\textbackslash}url\{https://github.com/arkel23/GLSim\}.},
	urldate = {2024-07-19},
	publisher = {arXiv},
	author = {Rios, Edwin Arkel and Hu, Min-Chun and Lai, Bo-Cheng},
	month = jul,
	year = {2024},
	note = {arXiv:2407.12891 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.2, I.4},
	annote = {Comment: Main: 12 pages, 5 figures, 5 tables. Appendix: 9 pages, 9 figures, 10 tables. Total: 21 pages, 14 figures, 15 tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\MH2L9C7S\\Rios et al. - 2024 - Global-Local Similarity for Efficient Fine-Grained.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\IYYK6NSA\\2407.html:text/html},
}

@misc{haurum_which_2023,
	title = {Which {Tokens} to {Use}? {Investigating} {Token} {Reduction} in {Vision} {Transformers}},
	shorttitle = {Which {Tokens} to {Use}?},
	abstract = {Since the introduction of the Vision Transformer (ViT), researchers have sought to make ViTs more efficient by removing redundant information in the processed tokens. While different methods have been explored to achieve this goal, we still lack understanding of the resulting reduction patterns and how those patterns differ across token reduction methods and datasets. To close this gap, we set out to understand the reduction patterns of 10 different token reduction methods using four image classification datasets. By systematically comparing these methods on the different classification tasks, we find that the Top-K pruning method is a surprisingly strong baseline. Through in-depth analysis of the different methods, we determine that: the reduction patterns are generally not consistent when varying the capacity of the backbone model, the reduction patterns of pruning-based methods significantly differ from fixed radial patterns, and the reduction patterns of pruning-based methods are correlated across classification datasets. Finally we report that the similarity of reduction patterns is a moderate-to-strong proxy for model performance. Project page at https://vap.aau.dk/tokens.},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Haurum, Joakim Bruslund and Escalera, Sergio and Taylor, Graham W. and Moeslund, Thomas B.},
	month = aug,
	year = {2023},
	doi = {10.48550/arXiv.2308.04657},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv:2308.04657 [cs]},
	annote = {Comment: ICCV 2023 NIVT Workshop. Project webpage https://vap.aau.dk/tokens},
}

@misc{tang_patch_2022,
	title = {Patch {Slimming} for {Efficient} {Vision} {Transformers}},
	abstract = {This paper studies the efficiency problem for visual transformers by excavating redundant calculation in given networks. The recent transformer architecture has demonstrated its effectiveness for achieving excellent performance on a series of computer vision tasks. However, similar to that of convolutional neural networks, the huge computational cost of vision transformers is still a severe issue. Considering that the attention mechanism aggregates different patches layer-by-layer, we present a novel patch slimming approach that discards useless patches in a top-down paradigm. We first identify the effective patches in the last layer and then use them to guide the patch selection process of previous layers. For each layer, the impact of a patch on the final output feature is approximated and patches with less impact will be removed. Experimental results on benchmark datasets demonstrate that the proposed method can significantly reduce the computational costs of vision transformers without affecting their performances. For example, over 45\% FLOPs of the ViT-Ti model can be reduced with only 0.2\% top-1 accuracy drop on the ImageNet dataset.},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Tang, Yehui and Han, Kai and Wang, Yunhe and Xu, Chang and Guo, Jianyuan and Xu, Chao and Tao, Dacheng},
	month = apr,
	year = {2022},
	doi = {10.48550/arXiv.2106.02852},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv:2106.02852 [cs]},
	annote = {Comment: This paper is accepted by CVPR 2022},
}

@misc{rao_dynamicvit_2021,
	title = {{DynamicViT}: {Efficient} {Vision} {Transformers} with {Dynamic} {Token} {Sparsification}},
	shorttitle = {{DynamicViT}},
	abstract = {Attention is sparse in vision transformers. We observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, we propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. Benefiting from the nature of self-attention, the unstructured sparse tokens are still hardware friendly, which makes our framework easy to achieve actual speed-up. By hierarchically pruning 66\% of the input tokens, our method greatly reduces 31\%{\textbackslash}textasciitilde37\% FLOPs and improves the throughput by over 40\% while the drop of accuracy is within 0.5\% for various vision transformers. Equipped with the dynamic token sparsification framework, DynamicViT models can achieve very competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs and vision transformers on ImageNet. Code is available at https://github.com/raoyongming/DynamicViT},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui},
	month = oct,
	year = {2021},
	doi = {10.48550/arXiv.2106.02034},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {arXiv:2106.02034 [cs]},
	annote = {Comment: Accepted to NeurIPS 2021. Project page: https://dynamicvit.ivg-research.xyz/},
}

@inproceedings{berg_poof_2013,
	title = {{POOF}: {Part}-{Based} {One}-vs.-{One} {Features} for {Fine}-{Grained} {Categorization}, {Face} {Verification}, and {Attribute} {Estimation}},
	shorttitle = {{POOF}},
	doi = {10.1109/CVPR.2013.128},
	abstract = {From a set of images in a particular domain, labeled with part locations and class, we present a method to automatically learn a large and diverse set of highly discriminative intermediate features that we call Part-based One-vs.-One Features (POOFs). Each of these features specializes in discrimination between two particular classes based on the appearance at a particular part. We demonstrate the particular usefulness of these features for fine-grained visual categorization with new state-of-the-art results on bird species identification using the Caltech UCSD Birds (CUB) dataset and parity with the best existing results in face verification on the Labeled Faces in the Wild (LFW) dataset. Finally, we demonstrate the particular advantage of POOFs when training data is scarce.},
	urldate = {2024-06-26},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Berg, Thomas and Belhumeur, Peter N.},
	month = jun,
	year = {2013},
	keywords = {Training, Image color analysis, Face, Feature extraction, Accuracy, Birds, Histograms, attributes, face verification, fine-grained visual categorization, part-based recognition},
	pages = {955--962},
	annote = {ISSN: 1063-6919},
}

@misc{huang_part-stacked_2015,
	title = {Part-{Stacked} {CNN} for {Fine}-{Grained} {Visual} {Categorization}},
	abstract = {In the context of fine-grained visual categorization, the ability to interpret models as human-understandable visual manuals is sometimes as important as achieving high classification accuracy. In this paper, we propose a novel Part-Stacked CNN architecture that explicitly explains the fine-grained recognition process by modeling subtle differences from object parts. Based on manually-labeled strong part annotations, the proposed architecture consists of a fully convolutional network to locate multiple object parts and a two-stream classification network that en- codes object-level and part-level cues simultaneously. By adopting a set of sharing strategies between the computation of multiple object parts, the proposed architecture is very efficient running at 20 frames/sec during inference. Experimental results on the CUB-200-2011 dataset reveal the effectiveness of the proposed architecture, from both the perspective of classification accuracy and model interpretability.},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Huang, Shaoli and Xu, Zhe and Tao, Dacheng and Zhang, Ya},
	month = dec,
	year = {2015},
	doi = {10.48550/arXiv.1512.08086},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv:1512.08086 [cs]},
}

@inproceedings{xie_hierarchical_2013,
	title = {Hierarchical {Part} {Matching} for {Fine}-{Grained} {Visual} {Categorization}},
	doi = {10.1109/ICCV.2013.206},
	abstract = {As a special topic in computer vision, fine-grained visual categorization (FGVC) has been attracting growing attention these years. Different with traditional image classification tasks in which objects have large inter-class variation, the visual concepts in the fine-grained datasets, such as hundreds of bird species, often have very similar semantics. Due to the large inter-class similarity, it is very difficult to classify the objects without locating really discriminative features, therefore it becomes more important for the algorithm to make full use of the part information in order to train a robust model. In this paper, we propose a powerful flowchart named Hierarchical Part Matching (HPM) to cope with fine-grained classification tasks. We extend the Bag-of-Features (BoF) model by introducing several novel modules to integrate into image representation, including foreground inference and segmentation, Hierarchical Structure Learning (HSL), and Geometric Phrase Pooling (GPP). We verify in experiments that our algorithm achieves the state-of-the-art classification accuracy in the Caltech-UCSD-Birds-200-2011 dataset by making full use of the ground-truth part annotations.},
	urldate = {2024-06-26},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Xie, Lingxi and Tian, Qi and Hong, Richang and Yan, Shuicheng and Zhang, Bo},
	month = dec,
	year = {2013},
	keywords = {Image segmentation, Semantics, Visualization, Birds, Vectors, Fine-Grained Visual Categorization, Foreground Inference and Segmentation, Geometric Phrase Pooling, Hierarchical Part Matching, Hierarchical Structure Learning, Inference algorithms, Legged locomotion},
	pages = {1641--1648},
	annote = {ISSN: 2380-7504},
}

@article{zhao_diversified_2017,
	title = {Diversified {Visual} {Attention} {Networks} for {Fine}-{Grained} {Object} {Classification}},
	volume = {19},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1520-9210, 1941-0077},
	doi = {10.1109/TMM.2017.2648498},
	abstract = {Fine-grained object classification attracts increasing attention in multimedia applications. However, it is a quite challenging problem due to the subtle interclass difference and large intraclass variation. Recently, visual attention models have been applied to automatically localize the discriminative regions of an image for better capturing critical difference, which have demonstrated promising performance. Unfortunately, without consideration of the diversity in attention process, most of existing attention models perform poorly in classifying fine-grained objects. In this paper, we propose a diversified visual attention network (DVAN) to address the problem of fine-grained object classification, which substantially relieves the dependency on strongly supervised information for learning to localize discriminative regions com-pared with attention-less models. More importantly, DVAN explicitly pursues the diversity of attention and is able to gather discriminative information to the maximal extent. Multiple attention canvases are generated to extract convolutional features for attention. An LSTM recurrent unit is employed to learn the attentiveness and discrimination of attention canvases. The proposed DVAN has the ability to attend the object from coarse to fine granularity, and a dynamic internal representation for classification is built up by incrementally combining the information from different locations and scales of the image. Extensive experiments conducted on CUB-2011, Stanford Dogs, and Stanford Cars datasets have demonstrated that the pro-posed DVAN achieves competitive performance compared to the state-of-the-art approaches, without using any prior knowledge, user interaction, or external resource in training and testing.},
	number = {6},
	urldate = {2024-06-26},
	journal = {IEEE Transactions on Multimedia},
	author = {Zhao, Bo and Wu, Xiao and Feng, Jiashi and Peng, Qiang and Yan, Shuicheng},
	month = jun,
	year = {2017},
	pages = {1245--1256},
	annote = {[TLDR] A diversified visual attention network (DVAN) is proposed to address the problem of fine-grained object classification, which substantially relieves the dependency on strongly supervised information for learning to localize discriminative regions com-pared with attention-less models.},
}

@inproceedings{ge_weakly_2019,
	address = {Long Beach, CA, USA},
	title = {Weakly {Supervised} {Complementary} {Parts} {Models} for {Fine}-{Grained} {Image} {Classification} {From} the {Bottom} {Up}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72813-293-8},
	doi = {10.1109/CVPR.2019.00315},
	abstract = {Given a training dataset composed of images and corresponding category labels, deep convolutional neural networks show a strong ability in mining discriminative parts for image classiﬁcation. However, deep convolutional neural networks trained with image level labels only tend to focus on the most discriminative parts while missing other object parts, which could provide complementary information. In this paper, we approach this problem from a different perspective. We build complementary parts models in a weakly supervised manner to retrieve information suppressed by dominant object parts detected by convolutional neural networks. Given image level labels only, we ﬁrst extract rough object instances by performing weakly supervised object detection and instance segmentation using Mask R-CNN and CRF-based segmentation. Then we estimate and search for the best parts model for each object instance under the principle of preserving as much diversity as possible. In the last stage, we build a bi-directional long short-term memory (LSTM) network to fuze and encode the partial information of these complementary parts into a comprehensive feature for image classiﬁcation. Experimental results indicate that the proposed method not only achieves signiﬁcant improvement over our baseline models, but also outperforms stateof-the-art algorithms by a large margin (6.7\%, 2.8\%, 5.2\% respectively) on Stanford Dogs 120, Caltech-UCSD Birds 2011-200 and Caltech 256.},
	language = {en},
	urldate = {2024-06-26},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ge, Weifeng and Lin, Xiangru and Yu, Yizhou},
	month = jun,
	year = {2019},
	pages = {3029--3038},
}

@misc{xiao_application_2014,
	title = {The {Application} of {Two}-level {Attention} {Models} in {Deep} {Convolutional} {Neural} {Network} for {Fine}-grained {Image} {Classification}},
	abstract = {Fine-grained classification is challenging because categories can only be discriminated by subtle and local differences. Variances in the pose, scale or rotation usually make the problem more difficult. Most fine-grained classification systems follow the pipeline of finding foreground object or object parts (where) to extract discriminative features (what). In this paper, we propose to apply visual attention to fine-grained classification task using deep neural network. Our pipeline integrates three types of attention: the bottom-up attention that propose candidate patches, the object-level top-down attention that selects relevant patches to a certain object, and the part-level top-down attention that localizes discriminative parts. We combine these attentions to train domain-specific deep nets, then use it to improve both the what and where aspects. Importantly, we avoid using expensive annotations like bounding box or part information from end-to-end. The weak supervision constraint makes our work easier to generalize. We have verified the effectiveness of the method on the subsets of ILSVRC2012 dataset and CUB200\_2011 dataset. Our pipeline delivered significant improvements and achieved the best accuracy under the weakest supervision condition. The performance is competitive against other methods that rely on additional annotations.},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Xiao, Tianjun and Xu, Yichong and Yang, Kuiyuan and Zhang, Jiaxing and Peng, Yuxin and Zhang, Zheng},
	month = nov,
	year = {2014},
	doi = {10.48550/arXiv.1411.6447},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv:1411.6447 [cs]},
}

@misc{zheng_rethinking_2021,
	title = {Rethinking {Semantic} {Segmentation} from a {Sequence}-to-{Sequence} {Perspective} with {Transformers}},
	abstract = {Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (ie, without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28\% mIoU), Pascal Context (55.83\% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip H. S. and Zhang, Li},
	month = jul,
	year = {2021},
	doi = {10.48550/arXiv.2012.15840},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv:2012.15840 [cs]},
	annote = {Comment: CVPR 2021. Project page at https://fudan-zvg.github.io/SETR/},
}

@misc{zong_self-slimmed_2022,
	title = {Self-slimmed {Vision} {Transformer}},
	abstract = {Vision transformers (ViTs) have become the popular structures and outperformed convolutional neural networks (CNNs) on various vision tasks. However, such powerful transformers bring a huge computation burden, because of the exhausting token-to-token comparison. The previous works focus on dropping insignificant tokens to reduce the computational cost of ViTs. But when the dropping ratio increases, this hard manner will inevitably discard the vital tokens, which limits its efficiency. To solve the issue, we propose a generic self-slimmed learning approach for vanilla ViTs, namely SiT. Specifically, we first design a novel Token Slimming Module (TSM), which can boost the inference efficiency of ViTs by dynamic token aggregation. As a general method of token hard dropping, our TSM softly integrates redundant tokens into fewer informative ones. It can dynamically zoom visual attention without cutting off discriminative token relations in the images, even with a high slimming ratio. Furthermore, we introduce a concise Feature Recalibration Distillation (FRD) framework, wherein we design a reverse version of TSM (RTSM) to recalibrate the unstructured token in a flexible auto-encoder manner. Due to the similar structure between teacher and student, our FRD can effectively leverage structure knowledge for better convergence. Finally, we conduct extensive experiments to evaluate our SiT. It demonstrates that our method can speed up ViTs by 1.7x with negligible accuracy drop, and even speed up ViTs by 3.6x while maintaining 97\% of their performance. Surprisingly, by simply arming LV-ViT with our SiT, we achieve new state-of-the-art performance on ImageNet. Code is available at https://github.com/Sense-X/SiT.},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Zong, Zhuofan and Li, Kunchang and Song, Guanglu and Wang, Yali and Qiao, Yu and Leng, Biao and Liu, Yu},
	month = sep,
	year = {2022},
	doi = {10.48550/arXiv.2111.12624},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv:2111.12624 [cs]},
	annote = {Comment: Accepted by ECCV 2022. Code is available at https://github.com/Sense-X/SiT},
}

@inproceedings{lee_token_2023,
	title = {Token {Adaptive} {Vision} {Transformer} with {Efficient} {Deployment} for {Fine}-{Grained} {Image} {Recognition}},
	doi = {10.23919/DATE56975.2023.10137239},
	abstract = {Fine-grained Visual Classification (FGVC) aims to distinguish object classes belonging to the same category, e.g., different bird species or models of vehicles. The task is more challenging than ordinary image classification due to the subtle inter-class differences. Recent works proposed deep learning models based on the vision transformer (ViT) architecture with its self-attention mechanism to locate important regions of the objects and derive global information. However, deploying them on resource-restricted internet of things (IoT) devices is challenging due to their intensive computational cost and memory footprint. Energy and power consumption varies in different IoT devices. To improve their inference efficiency, previous approaches require manually designing the model architecture and training a separate model for each computational budget. In this work, we propose Token Adaptive Vision Transformer (TAVT) that dynamically drops out tokens and can be used for various inference scenarios across many IoT devices after training the model once. Our adaptive model can switch among different token drop configurations at run time, providing instant accuracy-efficiency trade-offs. We train a vision transformer with a progressive token pruning scheme, eliminating a large number of redundant tokens in the later layers. We then conduct a multi-objective evolutionary search with the overall number of floating point operations (FLOPs) as its efficiency constraint that could be translated to energy consumption and power to find the token pruning schemes that maximize accuracy and efficiency under various computational budgets. Empirical results show that our proposed TAVT dramatically speeds up the GPU inference latency by up to 10× and reduces memory requirements and FLOPs by up to 5.5 × and 13 × respectively while achieving competitive accuracy compared to prior ViT-based state-of-the-art approaches.},
	urldate = {2024-07-19},
	booktitle = {2023 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Lee, Chonghan and Brufau, Rita Brugarolas and Ding, Ke and Narayanan, Vijaykrishnan},
	month = apr,
	year = {2023},
	note = {ISSN: 1558-1101},
	keywords = {Computational modeling, Training, Computational efficiency, Visualization, Adaptation models, Transformers, Switches},
	pages = {1--6},
}

@article{ma_fine-grained_2021,
	title = {Fine-{Grained} {Pests} {Recognition} {Based} on {Truncated} {Probability} {Fusion} {Network} via {Internet} of {Things} in {Forestry} and {Agricultural} {Scenes}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-4893},
	doi = {10.3390/a14100290},
	abstract = {Accurate identification of insect pests is the key to improve crop yield and ensure quality and safety. However, under the influence of environmental conditions, the same kind of pests show obvious differences in intraclass representation, while the different kinds of pests show slight similarities. The traditional methods have been difficult to deal with fine-grained identification of pests, and their practical deployment is low. In order to solve this problem, this paper uses a variety of equipment terminals in the agricultural Internet of Things to obtain a large number of pest images and proposes a fine-grained identification model of pests based on probability fusion network FPNT. This model designs a fine-grained feature extractor based on an optimized CSPNet backbone network, mining different levels of local feature expression that can distinguish subtle differences. After the integration of the NetVLAD aggregation layer, the gated probability fusion layer gives full play to the advantages of information complementarity and confidence coupling of multi-model fusion. The comparison test shows that the PFNT model has an average recognition accuracy of 93.18\% for all kinds of pests, and its performance is better than other deep-learning methods, with the average processing time drop to 61 ms, which can meet the needs of fine-grained image recognition of pests in the Internet of Things in agricultural and forestry practice, and provide technical application reference for intelligent early warning and prevention of pests.},
	language = {en},
	number = {10},
	urldate = {2024-07-25},
	journal = {Algorithms},
	author = {Ma, Kai and Nie, Ming-Jun and Lin, Sen and Kong, Jianlei and Yang, Cheng-Cai and Liu, Jinhao},
	month = oct,
	year = {2021},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {fine-grained visual classification, deep-learning neural network, gated probability fusion, insect pest recognition, soft-VLAD aggregation},
	pages = {290},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\FCN3G93M\\Ma et al. - 2021 - Fine-Grained Pests Recognition Based on Truncated .pdf:application/pdf},
}

@article{yang_fine-grained_2020,
	title = {Fine-{Grained} {Image} {Classification} for {Crop} {Disease} {Based} on {Attention} {Mechanism}},
	volume = {11},
	issn = {1664-462X},
	doi = {10.3389/fpls.2020.600854},
	abstract = {{\textless}p{\textgreater}Fine-grained image classification is a challenging task because of the difficulty in identifying discriminant features, it is not easy to find the subtle features that fully represent the object. In the fine-grained classification of crop disease, visual disturbances such as light, fog, overlap, and jitter are frequently encountered. To explore the influence of the features of crop leaf images on the classification results, a classification model should focus on the more discriminative regions of the image while improving the classification accuracy of the model in complex scenes. This paper proposes a novel attention mechanism that effectively utilizes the informative regions of an image, and describes the use of transfer learning to quickly construct several fine-grained image classification models of crop disease based on this attention mechanism. This study uses 58,200 crop leaf images as a dataset, including 14 different crops and 37 different categories of healthy/diseased crops. Among them, different diseases of the same crop have strong similarities. The NASNetLarge fine-grained classification model based on the proposed attention mechanism achieves the best classification effect, with an {\textless}italic{\textgreater}F{\textless}/italic{\textgreater}$_{\textrm{1}}$ score of up to 93.05\%. The results show that the proposed attention mechanism effectively improves the fine-grained classification of crop disease images.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-07-25},
	journal = {Frontiers in Plant Science},
	author = {Yang, Guofeng and He, Yong and Yang, Yong and Xu, Beibei},
	month = dec,
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {image classification, attention mechanism, crop disease, Fine-grained, Fine-tuning},
	file = {Full Text:C\:\\Users\\edwin\\Zotero\\storage\\ADIEDEDJ\\Yang et al. - 2020 - Fine-Grained Image Classification for Crop Disease.pdf:application/pdf},
}

@article{wang_identification_2021,
	title = {Identification of {Apple} {Leaf} {Diseases} by {Improved} {Deep} {Convolutional} {Neural} {Networks} {With} an {Attention} {Mechanism}},
	volume = {12},
	issn = {1664-462X},
	doi = {10.3389/fpls.2021.723294},
	abstract = {{\textless}p{\textgreater}The accurate identification of apple leaf diseases is of great significance for controlling the spread of diseases and ensuring the healthy and stable development of the apple industry. In order to improve detection accuracy and efficiency, a deep learning model, which is called the Coordination Attention EfficientNet (CA-ENet), is proposed to identify different apple diseases. First, a coordinate attention block is integrated into the EfficientNet-B4 network, which embedded the spatial location information of the feature by channel attention to ensure that the model can learn both the channel and spatial location information of important features. Then, a depth-wise separable convolution is applied to the convolution module to reduce the number of parameters, and the h-swish activation function is introduced to achieve the fast and easy to quantify the process. Afterward, 5,170 images are collected in the field environment at the apple planting base of the Northwest A\&amp;F University, while 3,000 images are acquired from the PlantVillage public data set. Also, image augmentation techniques are used to generate an Apple Leaf Disease Identification Data set (ALDID), which contains 81,700 images. The experimental results show that the accuracy of the CA-ENet is 98.92\% on the ALDID, and the average F1-score reaches .988, which is better than those of common models such as the ResNet-152, DenseNet-264, and ResNeXt-101. The generated test dataset is used to test the anti-interference ability of the model. The results show that the proposed method can achieve competitive performance on the apple disease identification task.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-07-25},
	journal = {Frontiers in Plant Science},
	author = {Wang, Peng and Niu, Tong and Mao, Yanru and Zhang, Zhao and Liu, Bin and He, Dongjian},
	month = sep,
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {attention mechanism, Apple disease, CA block, CA-ENet, Diseases identification},
	file = {Full Text:C\:\\Users\\edwin\\Zotero\\storage\\CI52FP9Z\\Wang et al. - 2021 - Identification of Apple Leaf Diseases by Improved .pdf:application/pdf},
}

@misc{bolya_token_2023,
	title = {Token {Merging}: {Your} {ViT} {But} {Faster}},
	shorttitle = {Token {Merging}},
	doi = {10.48550/arXiv.2210.09461},
	abstract = {We introduce Token Merging (ToMe), a simple method to increase the throughput of existing ViT models without needing to train. ToMe gradually combines similar tokens in a transformer using a general and light-weight matching algorithm that is as fast as pruning while being more accurate. Off-the-shelf, ToMe can 2x the throughput of state-of-the-art ViT-L @ 512 and ViT-H @ 518 models on images and 2.2x the throughput of ViT-L on video with only a 0.2-0.3\% accuracy drop in each case. ToMe can also easily be applied during training, improving in practice training speed up to 2x for MAE fine-tuning on video. Training with ToMe further minimizes accuracy drop, leading to 2x the throughput of ViT-B on audio for only a 0.4\% mAP drop. Qualitatively, we find that ToMe merges object parts into one token, even over multiple frames of video. Overall, ToMe's accuracy and speed are competitive with state-of-the-art on images, video, and audio.},
	urldate = {2024-07-25},
	publisher = {arXiv},
	author = {Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy},
	month = mar,
	year = {2023},
	note = {arXiv:2210.09461 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted ICLR 2023 Oral (top 5\%) [final v2]. This version includes stable diffusion experiments. See code at https://github.com/facebookresearch/ToMe},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\WZH32YIM\\Bolya et al. - 2023 - Token Merging Your ViT But Faster.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\IRW93SMS\\2210.html:text/html},
}

@misc{fayyaz_adaptive_2022,
	title = {Adaptive {Token} {Sampling} {For} {Efficient} {Vision} {Transformers}},
	doi = {10.48550/arXiv.2111.15667},
	abstract = {While state-of-the-art vision transformer models achieve promising results in image classification, they are computationally expensive and require many GFLOPs. Although the GFLOPs of a vision transformer can be decreased by reducing the number of tokens in the network, there is no setting that is optimal for all input images. In this work, we therefore introduce a differentiable parameter-free Adaptive Token Sampler (ATS) module, which can be plugged into any existing vision transformer architecture. ATS empowers vision transformers by scoring and adaptively sampling significant tokens. As a result, the number of tokens is not constant anymore and varies for each input image. By integrating ATS as an additional layer within the current transformer blocks, we can convert them into much more efficient vision transformers with an adaptive number of tokens. Since ATS is a parameter-free module, it can be added to the off-the-shelf pre-trained vision transformers as a plug and play module, thus reducing their GFLOPs without any additional training. Moreover, due to its differentiable design, one can also train a vision transformer equipped with ATS. We evaluate the efficiency of our module in both image and video classification tasks by adding it to multiple SOTA vision transformers. Our proposed module improves the SOTA by reducing their computational costs (GFLOPs) by 2X, while preserving their accuracy on the ImageNet, Kinetics-400, and Kinetics-600 datasets.},
	urldate = {2024-07-25},
	publisher = {arXiv},
	author = {Fayyaz, Mohsen and Koohpayegani, Soroush Abbasi and Jafari, Farnoush Rezaei and Sengupta, Sunando and Joze, Hamid Reza Vaezi and Sommerlade, Eric and Pirsiavash, Hamed and Gall, Juergen},
	month = jul,
	year = {2022},
	note = {arXiv:2111.15667 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ECCV 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\BPMRXJMC\\Fayyaz et al. - 2022 - Adaptive Token Sampling For Efficient Vision Trans.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\3XK6J9ZG\\2111.html:text/html},
}

@article{du_study_2016,
	title = {Study on density peaks clustering based on k-nearest neighbors and principal component analysis},
	volume = {99},
	issn = {0950-7051},
	doi = {10.1016/j.knosys.2016.02.001},
	abstract = {Density peaks clustering (DPC) algorithm published in the US journal Science in 2014 is a novel clustering algorithm based on density. It needs neither iterative process nor more parameters. However, original algorithm only has taken into account the global structure of data, which leads to missing many clusters. In addition, DPC does not perform well when data sets have relatively high dimension. Especially, DPC generates wrong number of clusters of real-world data sets. In order to overcome the first problem, we propose a density peaks clustering based on k nearest neighbors (DPC-KNN) which introduces the idea of k nearest neighbors (KNN) into DPC and has another option for the local density computation. In order to overcome the second problem, we introduce principal component analysis (PCA) into the model of DPC-KNN and further bring forward a method based on PCA (DPC-KNN-PCA), which preprocesses high-dimensional data. By experiments on synthetic data sets, we demonstrate the feasibility of our algorithms. By experiments on real-world data sets, we compared this algorithm with k-means algorithm and spectral clustering (SC) algorithm in accuracy. Experimental results show that our algorithms are feasible and effective.},
	urldate = {2024-07-25},
	journal = {Knowledge-Based Systems},
	author = {Du, Mingjing and Ding, Shifei and Jia, Hongjie},
	month = may,
	year = {2016},
	keywords = {Data clustering, Density peaks, k Nearest neighbors (KNN), Principal component analysis (PCA)},
	pages = {135--145},
	file = {ScienceDirect Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\SWI3SWMN\\S0950705116000794.html:text/html},
}

@misc{renggli_learning_2022,
	title = {Learning to {Merge} {Tokens} in {Vision} {Transformers}},
	doi = {10.48550/arXiv.2202.12015},
	abstract = {Transformers are widely applied to solve natural language understanding and computer vision tasks. While scaling up these architectures leads to improved performance, it often comes at the expense of much higher computational costs. In order for large-scale models to remain practical in real-world systems, there is a need for reducing their computational overhead. In this work, we present the PatchMerger, a simple module that reduces the number of patches or tokens the network has to process by merging them between two consecutive intermediate layers. We show that the PatchMerger achieves a significant speedup across various model sizes while matching the original performance both upstream and downstream after fine-tuning.},
	urldate = {2024-07-25},
	publisher = {arXiv},
	author = {Renggli, Cedric and Pinto, André Susano and Houlsby, Neil and Mustafa, Basil and Puigcerver, Joan and Riquelme, Carlos},
	month = feb,
	year = {2022},
	note = {arXiv:2202.12015 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 11 pages, 9 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\HY4FNLT3\\Renggli et al. - 2022 - Learning to Merge Tokens in Vision Transformers.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\L92EMWJT\\2202.html:text/html},
}

@inproceedings{zeng_not_2022,
	address = {New Orleans, LA, USA},
	title = {Not {All} {Tokens} {Are} {Equal}: {Human}-centric {Visual} {Analysis} via {Token} {Clustering} {Transformer}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-946-3},
	shorttitle = {Not {All} {Tokens} {Are} {Equal}},
	doi = {10.1109/CVPR52688.2022.01082},
	abstract = {Vision transformers have achieved great successes in many computer vision tasks. Most methods generate vision tokens by splitting an image into a regular and fixed grid and treating each cell as a token. However, not all regions are equally important in human-centric vision tasks, e.g., the human body needs a fine representation with many tokens, while the image background can be modeled by a few tokens. To address this problem, we propose a novel Vision Transformer, called Token Clustering Transformer (TCFormer), which merges tokens by progressive clustering, where the tokens can be merged from different locations with flexible shapes and sizes. The tokens in TCFormer can not only focus on important areas but also adjust the token shapes to fit the semantic concept and adopt a fine resolution for regions containing critical details, which is beneficial to capturing detailed information. Extensive experiments show that TCFormer consistently outperforms its counterparts on different challenging human-centric tasks and datasets, including whole-body pose estimation on COCO-WholeBody and 3D human mesh reconstruction on 3DPW. Code is available at https://github.com/ zengwang430521/TCFormer.git.},
	language = {en},
	urldate = {2024-07-25},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zeng, Wang and Jin, Sheng and Liu, Wentao and Qian, Chen and Luo, Ping and Ouyang, Wanli and Wang, Xiaogang},
	month = jun,
	year = {2022},
	pages = {11091--11101},
	file = {Zeng et al. - 2022 - Not All Tokens Are Equal Human-centric Visual Ana.pdf:C\:\\Users\\edwin\\Zotero\\storage\\7ZFJDSMK\\Zeng et al. - 2022 - Not All Tokens Are Equal Human-centric Visual Ana.pdf:application/pdf},
}

@inproceedings{liang_evit_2021,
	title = {{EViT}: {Expediting} {Vision} {Transformers} via {Token} {Reorganizations}},
	shorttitle = {{EViT}},
	abstract = {Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them. Complete leverage of these image tokens brings redundant computations since not all the tokens are attentive in MHSA. Examples include that tokens containing semantically meaningless or distractive image backgrounds do not positively contribute to the ViT predictions. In this work, we propose to reorganize image tokens during the feed-forward process of ViT models, which is integrated into ViT during training. For each forward inference, we identify the attentive image tokens between MHSA and FFN (i.e., feed-forward network) modules, which is guided by the corresponding class token attention. Then, we reorganize image tokens by preserving attentive image tokens and fusing inattentive ones to expedite subsequent MHSA and FFN computations. To this end, our method EViT improves ViTs from two perspectives. First, under the same amount of input image tokens, our method reduces MHSA and FFN computation for efficient inference. For instance, the inference speed of DeiT-S is increased by 50\% while its recognition accuracy is decreased by only 0.3\% for ImageNet classification. Second, by maintaining the same computational cost, our method empowers ViTs to take more image tokens as input for recognition accuracy improvement, where the image tokens are from higher resolution images. An example is that we improve the recognition accuracy of DeiT-S by 1\% for ImageNet classification at the same computational cost of a vanilla DeiT-S. Meanwhile, our method does not introduce more parameters to ViTs. Experiments on the standard benchmarks show the effectiveness of our method. The code is available at https://github.com/youweiliang/evit},
	language = {en},
	urldate = {2024-07-25},
	author = {Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
	month = oct,
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\N98CTH6S\\Liang et al. - 2021 - EViT Expediting Vision Transformers via Token Reo.pdf:application/pdf},
}

@inproceedings{he_masked_2022,
	address = {New Orleans, LA, USA},
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-946-3},
	doi = {10.1109/CVPR52688.2022.01553},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we ﬁnd that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efﬁciently and effectively: we accelerate training (by 3× or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.},
	language = {en},
	urldate = {2024-07-25},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollar, Piotr and Girshick, Ross},
	month = jun,
	year = {2022},
	pages = {15979--15988},
	file = {He et al. - 2022 - Masked Autoencoders Are Scalable Vision Learners.pdf:C\:\\Users\\edwin\\Zotero\\storage\\ZXDFJZ59\\He et al. - 2022 - Masked Autoencoders Are Scalable Vision Learners.pdf:application/pdf},
}

@article{ridnik_imagenet-21k_2021,
	title = {{ImageNet}-{21K} {Pretraining} for the {Masses}},
	volume = {1},
	language = {en},
	urldate = {2024-07-25},
	journal = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
	author = {Ridnik, Tal and Ben-Baruch, Emanuel and Noy, Asaf and Zelnik, Lihi},
	month = dec,
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\WARLEG7E\\Ridnik et al. - 2021 - ImageNet-21K Pretraining for the Masses.pdf:application/pdf},
}

@inproceedings{chen_empirical_2021,
	address = {Montreal, QC, Canada},
	title = {An {Empirical} {Study} of {Training} {Self}-{Supervised} {Vision} {Transformers}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66542-812-5},
	doi = {10.1109/ICCV48922.2021.00950},
	abstract = {This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: selfsupervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other selfsupervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.},
	language = {en},
	urldate = {2024-07-25},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Chen, Xinlei and Xie, Saining and He, Kaiming},
	month = oct,
	year = {2021},
	pages = {9620--9629},
	file = {Chen et al. - 2021 - An Empirical Study of Training Self-Supervised Vis.pdf:C\:\\Users\\edwin\\Zotero\\storage\\G3HY4UUZ\\Chen et al. - 2021 - An Empirical Study of Training Self-Supervised Vis.pdf:application/pdf},
}

@inproceedings{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	doi = {10.1109/ICCV48922.2021.00951},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [16] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [26], multi-crop training [9], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2024-07-25},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jegou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {Computer architecture, Training, Computer vision, Image segmentation, Semantics, Image retrieval, Layout, Recognition and classification, Representation learning, Transfer/Low-shot/Semi/Unsupervised Learning},
	pages = {9630--9640},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\89N55RQF\\9709990.html:text/html;Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\CPRNWBW4\\Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf:application/pdf},
}

@article{schuhmann_laion-5b_2022,
	title = {{LAION}-{5B}: {An} open large-scale dataset for training next generation image-text models},
	volume = {35},
	shorttitle = {{LAION}-{5B}},
	language = {en},
	urldate = {2024-07-25},
	journal = {Advances in Neural Information Processing Systems},
	author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
	month = dec,
	year = {2022},
	pages = {25278--25294},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\7CCP8K2F\\Schuhmann et al. - 2022 - LAION-5B An open large-scale dataset for training.pdf:application/pdf},
}

@inproceedings{kornblith_better_2019,
	address = {Long Beach, CA, USA},
	title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72813-293-8},
	doi = {10.1109/CVPR.2019.00277},
	abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classiﬁcation networks on 12 image classiﬁcation datasets. We ﬁnd that, when networks are used as ﬁxed feature extractors or ﬁne-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we ﬁnd that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we ﬁnd that, on two small ﬁne-grained image classiﬁcation datasets, pretraining on ImageNet provides minimal beneﬁts, indicating the learned features from ImageNet do not transfer well to ﬁne-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
	language = {en},
	urldate = {2024-07-25},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
	month = jun,
	year = {2019},
	pages = {2656--2666},
	file = {Kornblith et al. - 2019 - Do Better ImageNet Models Transfer Better.pdf:C\:\\Users\\edwin\\Zotero\\storage\\YGDX5BXX\\Kornblith et al. - 2019 - Do Better ImageNet Models Transfer Better.pdf:application/pdf},
}

@inproceedings{evci_head2toe_2022,
	title = {{Head2Toe}: {Utilizing} {Intermediate} {Representations} for {Better} {Transfer} {Learning}},
	shorttitle = {{Head2Toe}},
	abstract = {Transfer-learning methods aim to improve performance in a data-scarce target domain using a model pretrained on a data-rich source domain. A cost-efficient strategy, linear probing, involves freezing the source model and training a new classification head for the target domain. This strategy is outperformed by a more costly but state-of-the-art method – fine-tuning all parameters of the source model to the target domain – possibly because fine-tuning allows the model to leverage useful information from intermediate layers which is otherwise discarded by the later previously trained layers. We explore the hypothesis that these intermediate layers might be directly exploited. We propose a method, Head-to-Toe probing (Head2Toe), that selects features from all layers of the source model to train a classification head for the target-domain. In evaluations on the Visual Task Adaptation Benchmark-1k, Head2Toe matches performance obtained with fine-tuning on average while reducing training and storage cost hundred folds or more, but critically, for out-of-distribution transfer, Head2Toe outperforms fine-tuning. Code used in our experiments can be found in supplementary materials.},
	language = {en},
	urldate = {2024-07-25},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Evci, Utku and Dumoulin, Vincent and Larochelle, Hugo and Mozer, Michael C.},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {6009--6033},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\HS77R6MR\\Evci et al. - 2022 - Head2Toe Utilizing Intermediate Representations f.pdf:application/pdf},
}

@inproceedings{yu_benchmark_2021,
	title = {Benchmark {Platform} for {Ultra}-{Fine}-{Grained} {Visual} {Categorization} {Beyond} {Human} {Performance}},
	doi = {10.1109/ICCV48922.2021.01012},
	abstract = {Deep learning methods have achieved remarkable success in fine-grained visual categorization. Such successful categorization at sub-ordinate level, e.g., different animal or plant species, however relies heavily on the visual differences that human can observe and the ground-truths are labelled on the basis of such human visual observation. In contrast, few research has been done for visual categorization at the ultra-fine-grained level, i.e., a granularity where even human experts can hardly identify the visual differences or are not yet able to give affirmative labels by inferring observed pattern differences. This paper reports our efforts towards mitigating this research gap. We introduce the ultra-fine-grained (UFG) image dataset, a large collection of 47,114 images from 3,526 categories. All the images in the proposed UFG image dataset are grouped into categories with different confirmed cultivar names. In addition, we perform an extensive evaluation of state-of-the-art fine-grained classification methods on the proposed UFG image dataset as comparative baselines. The proposed UFG image dataset and evaluation protocols is intended to serve as a benchmark platform that can advance research of visual classification from approaching human performance to beyond human ability, via facilitating benchmark data of artificial intelligence (AI) not to be limited by the labels of human intelligence (HI). The dataset is available online at https://githuh.com/XiaohanYu-GU/Ultra-FGVC.},
	urldate = {2024-07-25},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Yu, Xiaohan and Zhao, Yang and Gao, Yongsheng and Yuan, Xiaohui and Xiong, Shengwu},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {and cell microscopy, Annotations, Benchmark testing, biological, Computer vision, Datasets and evaluation, Deep learning, Medical, Protocols, Recognition and classification, Training, Visualization},
	pages = {10265--10275},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\8AEJYC2I\\9710088.html:text/html},
}

@misc{howard_mobilenets_2017,
	title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	shorttitle = {{MobileNets}},
	doi = {10.48550/arXiv.1704.04861},
	abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	urldate = {2024-07-25},
	publisher = {arXiv},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	month = apr,
	year = {2017},
	note = {arXiv:1704.04861 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\5ARMECWG\\Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\QDLVDVVI\\1704.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2024-07-25},
	journal = {Neural Comput.},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@article{chang_devil_2020,
	title = {The {Devil} is in the {Channels}: {Mutual}-{Channel} {Loss} for {Fine}-{Grained} {Image} {Classification}},
	volume = {29},
	issn = {1057-7149},
	shorttitle = {The {Devil} is in the {Channels}},
	doi = {10.1109/TIP.2020.2973812},
	abstract = {The key to solving fine-grained image categorization is finding discriminate and local regions that correspond to subtle visual traits. Great strides have been made, with complex networks designed specifically to learn part-level discriminate feature representations. In this paper, we show that it is possible to cultivate subtle details without the need for overly complicated network designs or training mechanisms \&amp;\#x2013; a single loss is all it takes. The main trick lies with how we delve into individual feature channels early on, as opposed to the convention of starting from a consolidated feature map. The proposed loss function, termed as mutual-channel loss (MC-Loss), consists of two channel-specific components: a discriminality component and a diversity component. The discriminality component forces all feature channels belonging to the same class to be discriminative, through a novel channel-wise attention mechanism. The diversity component additionally constraints channels so that they become mutually exclusive across the spatial dimension. The end result is therefore a set of feature channels, each of which reflects different locally discriminative regions for a specific class. The MC-Loss can be trained end-to-end, without the need for any bounding-box/part annotations, and yields highly discriminative regions during inference. Experimental results show our MC-Loss when implemented on top of common base networks can achieve state-of-the-art performance on all four fine-grained categorization datasets (CUB-Birds, FGVC-Aircraft, Flowers-102, and Stanford Cars). Ablative studies further demonstrate the superiority of the MC-Loss when compared with other recently proposed general-purpose losses for visual classification, on two different base networks. Codes are available at: \&lt;uri\&gt;https://github.com/dongliangchang/Mutual-Channel-Loss\&lt;/uri\&gt;.},
	urldate = {2024-07-25},
	journal = {Trans. Img. Proc.},
	author = {Chang, Dongliang and Ding, Yifeng and Xie, Jiyang and Bhunia, Ayan Kumar and Li, Xiaoxu and Ma, Zhanyu and Wu, Ming and Guo, Jun and Song, Yi-Zhe},
	month = jan,
	year = {2020},
	pages = {4683--4695},
	file = {Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\265MCWBE\\Chang et al. - 2020 - The Devil is in the Channels Mutual-Channel Loss .pdf:application/pdf},
}

@article{gao_channel_2020,
	title = {Channel {Interaction} {Networks} for {Fine}-{Grained} {Image} {Categorization}},
	volume = {34},
	copyright = {https://www.aaai.org},
	issn = {2374-3468, 2159-5399},
	doi = {10.1609/aaai.v34i07.6712},
	abstract = {Fine-grained image categorization is challenging due to the subtle inter-class differences. We posit that exploiting the rich relationships between channels can help capture such differences since different channels correspond to different semantics. In this paper, we propose a channel interaction network (CIN), which models the channel-wise interplay both within an image and across images. For a single image, a self-channel interaction (SCI) module is proposed to explore channel-wise correlation within the image. This allows the model to learn the complementary features from the correlated channels, yielding stronger ﬁne-grained features. Furthermore, given an image pair, we introduce a contrastive channel interaction (CCI) module to model the cross-sample channel interaction with a metric learning framework, allowing the CIN to distinguish the subtle visual differences between images. Our model can be trained efﬁciently in an end-to-end fashion without the need of multi-stage training and testing. Finally, comprehensive experiments are conducted on three publicly available benchmarks, where the proposed method consistently outperforms the state-of-theart approaches, such as DFL-CNN(Wang, Morariu, and Davis 2018) and NTS(Yang et al. 2018).},
	language = {en},
	number = {07},
	urldate = {2024-07-25},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Gao, Yu and Han, Xintong and Wang, Xun and Huang, Weilin and Scott, Matthew},
	month = apr,
	year = {2020},
	pages = {10818--10825},
	file = {Gao et al. - 2020 - Channel Interaction Networks for Fine-Grained Imag.pdf:C\:\\Users\\edwin\\Zotero\\storage\\8LUB8ZE7\\Gao et al. - 2020 - Channel Interaction Networks for Fine-Grained Imag.pdf:application/pdf},
}

@article{yu_mix-vit_2023,
	title = {Mix-{ViT}: {Mixing} attentive vision transformer for ultra-fine-grained visual categorization},
	volume = {135},
	issn = {0031-3203},
	shorttitle = {Mix-{ViT}},
	doi = {10.1016/j.patcog.2022.109131},
	abstract = {Ultra-fine-grained visual categorization (ultra-FGVC) moves down the taxonomy level to classify sub-granularity categories of fine-grained objects. This inevitably poses a challenge, i.e., classifying highly similar objects with limited samples, which impedes the performance of recent advanced vision transformer methods. To that end, this paper introduces Mix-ViT, a novel mixing attentive vision transformer to address the above challenge towards improved ultra-FGVC. The core design is a self-supervised module that mixes the high-level sample tokens and learns to predict whether a token has been substituted after attentively substituting tokens. This drives the model to understand the contextual discriminative details among inter-class samples. Via incorporating such a self-supervised module, the network gains more knowledge from the intrinsic structure of input data and thus improves generalization capability with limited training sample. The proposed Mix-ViT achieves competitive performance on seven publicly available datasets, demonstrating the potential of vision transformer compared to CNN for the first time in addressing the challenging ultra-FGVC tasks. The code is available at https://github.com/Markin-Wang/MixViT},
	urldate = {2024-07-25},
	journal = {Pattern Recognition},
	author = {Yu, Xiaohan and Wang, Jun and Zhao, Yang and Gao, Yongsheng},
	month = mar,
	year = {2023},
	keywords = {Vision transformer, Attentive mixing, Self-supervised learning, Ultra-fine-grained visual categorization},
	pages = {109131},
}

@article{fang_learning_2024,
	title = {Learning {Contrastive} {Self}-{Distillation} for {Ultra}-{Fine}-{Grained} {Visual} {Categorization} {Targeting} {Limited} {Samples}},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2024.3370731},
	abstract = {In the field of intelligent multimedia analysis, ultra-fine-grained visual categorization (Ultra-FGVC) plays a vital role in distinguishing intricate subcategories within broader categories. However, this task is inherently challenging due to the complex granularity of category subdivisions and the limited availability of data for each category. To address these challenges, this work proposes CSDNet, a pioneering framework that effectively explores contrastive learning and self-distillation to learn discriminative representations specifically designed for Ultra-FGVC tasks. CSDNet comprises three main modules: Subcategory-Specific Discrepancy Parsing (SSDP), Dynamic Discrepancy Learning (DDL), and Subcategory-Specific Discrepancy Transfer (SSDT), which collectively enhance the generalization of deep models across instance, feature, and logit prediction levels. To increase the diversity of training samples, the SSDP module introduces adaptive augmented samples to spotlight subcategory-specific discrepancies. Simultaneously, the proposed DDL module stores historical intermediate features by a dynamic memory queue, which optimizes the feature learning space through iterative contrastive learning. Furthermore, the SSDT module effectively distills subcategory-specific discrepancies knowledge from the inherent structure of limited training data using a self-distillation paradigm at the logit prediction level. Experimental results demonstrate that CSDNet outperforms current state-of-the-art Ultra-FGVC methods, emphasizing its powerful efficacy and adaptability in addressing Ultra-FGVC tasks.},
	urldate = {2024-07-25},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Fang, Ziye and Jiang, Xin and Tang, Hao and Li, Zechao},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Circuits and systems, contrastive learning, Data augmentation, Feature extraction, self-distillation, Self-supervised learning, Task analysis, Training, Ultra-fine-grained visual categorization, Visualization},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\2JK7E8XA\\10445701.html:text/html;Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\IJSINRG3\\Fang et al. - 2024 - Learning Contrastive Self-Distillation for Ultra-F.pdf:application/pdf},
}

@inproceedings{yu_cle-vit_2023,
	address = {Macau, SAR China},
	title = {{CLE}-{ViT}: {Contrastive} {Learning} {Encoded} {Transformer} for {Ultra}-{Fine}-{Grained} {Visual} {Categorization}},
	isbn = {978-1-956792-03-4},
	shorttitle = {{CLE}-{ViT}},
	doi = {10.24963/ijcai.2023/504},
	abstract = {Ultra-fine-grained visual classification (ultra-FGVC) targets at classifying sub-grained categories of fine-grained objects. This inevitably requires discriminative representation learning within a limited training set. Exploring intrinsic features from the object itself, e.g., predicting the rotation of a given image, has demonstrated great progress towards learning discriminative representation. Yet none of these works consider explicit supervision for learning mutual information at instance level. To this end, this paper introduces CLE-ViT, a novel contrastive learning encoded transformer, to address the fundamental problem in ultra-FGVC. The core design is a self-supervised module that performs self-shuffling and masking and then distinguishes these altered images from other images. This drives the model to learn an optimized feature space that has a large inter-class distance while remaining tolerant to intra-class variations. By incorporating this self-supervised module, the network acquires more knowledge from the intrinsic structure of the input data, which improves the generalization ability without requiring extra manual annotations. CLE-ViT demonstrates strong performance on 7 publicly available datasets, demonstrating its effectiveness in the ultra-FGVC task. The code is available at https://github.com/Markin-Wang/CLEViT.},
	language = {en},
	urldate = {2024-07-25},
	booktitle = {Proceedings of the {Thirty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Yu, Xiaohan and Wang, Jun and Gao, Yongsheng},
	month = aug,
	year = {2023},
	pages = {4531--4539},
	file = {Yu et al. - 2023 - CLE-ViT Contrastive Learning Encoded Transformer .pdf:C\:\\Users\\edwin\\Zotero\\storage\\6UN3KBA3\\Yu et al. - 2023 - CLE-ViT Contrastive Learning Encoded Transformer .pdf:application/pdf},
}

@inproceedings{liu_swin_2021,
	address = {Montreal, QC, Canada},
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66542-812-5},
	shorttitle = {Swin {Transformer}},
	doi = {10.1109/ICCV48922.2021.00986},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efﬁciency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the ﬂexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classiﬁcation (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO testdev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-theart by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneﬁcial for all-MLP architectures. The code and models are publicly available at https://github. com/microsoft/Swin-Transformer.},
	language = {en},
	urldate = {2024-08-02},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = oct,
	year = {2021},
	pages = {9992--10002},
	file = {Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:C\:\\Users\\edwin\\Zotero\\storage\\2BMFGL5H\\Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:application/pdf},
}

@inproceedings{thompson_computational_2023,
	address = {Virtual},
	title = {The {Computational} {Limits} of {Deep} {Learning}},
	doi = {10.21428/bf6fb269.1f033948},
	language = {en},
	urldate = {2024-11-15},
	booktitle = {Ninth {Computing} within {Limits} 2023},
	publisher = {LIMITS},
	author = {Thompson, Neil and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F.},
	month = jun,
	year = {2023},
	file = {Thompson et al. - 2023 - The Computational Limits of Deep Learning.pdf:C\:\\Users\\edwin\\Zotero\\storage\\BHFHBWXF\\Thompson et al. - 2023 - The Computational Limits of Deep Learning.pdf:application/pdf},
}

@inproceedings{wang_spatten_2021,
	title = {{SpAtten}: {Efficient} {Sparse} {Attention} {Architecture} with {Cascade} {Token} and {Head} {Pruning}},
	shorttitle = {{SpAtten}},
	doi = {10.1109/HPCA51647.2021.00018},
	abstract = {The attention mechanism is becoming increasingly popular in Natural Language Processing (NLP) applications, showing superior performance than convolutional and recurrent architectures. However, general-purpose platforms such as CPUs and GPUs are inefficient when performing attention inference due to complicated data movement and low arithmetic intensity. Moreover, existing NN accelerators mainly focus on optimizing convolutional or recurrent models, and cannot efficiently support attention. In this paper, we present SpAtten, an efficient algorithm-architecture co-design that leverages token sparsity, head sparsity, and quantization opportunities to reduce the attention computation and memory access. Inspired by the high redundancy of human languages, we propose the novel cascade token pruning to prune away unimportant tokens in the sentence. We also propose cascade head pruning to remove unessential heads. Cascade pruning is fundamentally different from weight pruning since there is no trainable weight in the attention mechanism, and the pruned tokens and heads are selected on the fly. To efficiently support them on hardware, we design a novel top-k engine to rank token and head importance scores with high throughput. Furthermore, we propose progressive quantization that first fetches MSBs only and performs the computation; if the confidence is low, it fetches LSBs and recomputes the attention outputs, trading computation for memory reduction.Extensive experiments on 30 benchmarks show that, on average, SpAtten reduces DRAM access by 10.0× with no accuracy loss, and achieves 1.6×, 3.0×, 162×, 347× speedup, and 1.4×, 3.2×, 1193×, 4059× energy savings over A3 accelerator, MNNFast accelerator, TITAN Xp GPU, Xeon CPU, respectively.},
	urldate = {2024-11-21},
	booktitle = {2021 {IEEE} {International} {Symposium} on {High}-{Performance} {Computer} {Architecture} ({HPCA})},
	author = {Wang, Hanrui and Zhang, Zhekai and Han, Song},
	month = feb,
	year = {2021},
	note = {ISSN: 2378-203X},
	keywords = {Random access memory, Attention, Quantization, Natural language processing, Algorithm-Architecture Co-design, Domain-Specific Accelerator, Memory management, Natural Language Processing, Pruning, Quantization (signal), Redundancy, Space exploration, Throughput},
	pages = {97--110},
	file = {Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\2JQ9XL4E\\Wang et al. - 2021 - SpAtten Efficient Sparse Attention Architecture w.pdf:application/pdf},
}

@inproceedings{goyal_power-bert_2020,
	series = {{ICML}'20},
	title = {{PoWER}-{BERT}: accelerating {BERT} inference via progressive word-vector elimination},
	volume = {119},
	shorttitle = {{PoWER}-{BERT}},
	abstract = {We develop a novel method, called PoWER-BERT, for improving the inference time of the popular BERT model, while maintaining the accuracy. It works by: a) exploiting redundancy pertaining to word-vectors (intermediate transformer block outputs) and eliminating the redundant vectors. b) determining which wordvectors to eliminate by developing a strategy for measuring their significance, based on the self-attention mechanism. c) learning how many word-vectors to eliminate by augmenting the BERT model and the loss function. Experiments on the standard GLUE benchmark shows that PoWER-BERT achieves up to 4.5x reduction in inference time over BERT with \&lt; 1\% loss in accuracy. We show that PoWER-BERT offers significantly better trade-off between accuracy and inference time compared to prior methods. We demonstrate that our method attains up to 6.8x reduction in inference time with \&lt; 1\% loss in accuracy when applied over ALBERT, a highly compressed version of BERT.},
	urldate = {2024-11-21},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Goyal, Saurabh and Choudhury, Anamitra Roy and Raje, Saurabh M. and Chakaravarthy, Venkatesan T. and Sabharwal, Yogish and Verma, Ashish},
	month = jul,
	year = {2020},
	pages = {3690--3699},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\JBTME5H5\\Goyal et al. - 2020 - PoWER-BERT accelerating BERT inference via progre.pdf:application/pdf},
}

@inproceedings{fu_linguistic-aware_2024,
	address = {Seattle, WA, USA},
	title = {Linguistic-{Aware} {Patch} {Slimming} {Framework} for {Fine}-{Grained} {Cross}-{Modal} {Alignment}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350353006},
	doi = {10.1109/CVPR52733.2024.02485},
	abstract = {Cross-modal alignment aims to build a bridge connecting vision and language. It is an important multi-modal task that efficiently learns the semantic similarities between images and texts. Traditional fine-grained alignment methods heavily rely on pre-trained object detectors to extract region features for subsequent region-word alignment, thereby incurring substantial computational costs for region detection and error propagation issues for two-stage training. In this paper, we focus on the mainstream vision transformer, incorporating patch features for patch-word alignment, while addressing the resultant issue of visual patch redundancy and patch ambiguity for semantic alignment. We propose a novel Linguistic-Aware Patch Slimming (LAPS) framework for fine-grained alignment, which explicitly identifies redundant visual patches with language supervision and rectifies their semantic and spatial information to facilitate more effective and consistent patchword alignment. Extensive experiments on various evaluation benchmarks and model backbones show LAPS outperforms the state-of-the-art fine-grained alignment methods by 5\%-15\% rSum. Our code is available at https: //github.com/CrossmodalGroup/LAPS.},
	language = {en},
	urldate = {2024-11-21},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Fu, Zheren and Zhang, Lei and Xia, Hou and Mao, Zhendong},
	month = jun,
	year = {2024},
	pages = {26297--26306},
	file = {Fu et al. - 2024 - Linguistic-Aware Patch Slimming Framework for Fine.pdf:C\:\\Users\\edwin\\Zotero\\storage\\6APSG5TI\\Fu et al. - 2024 - Linguistic-Aware Patch Slimming Framework for Fine.pdf:application/pdf},
}

@article{he_why_2020,
	title = {Why {ResNet} {Works}? {Residuals} {Generalize}},
	volume = {31},
	issn = {2162-2388},
	shorttitle = {Why {ResNet} {Works}?},
	doi = {10.1109/TNNLS.2020.2966319},
	abstract = {Residual connections significantly boost the performance of deep neural networks. However, few theoretical results address the influence of residuals on the hypothesis complexity and the generalization ability of deep neural networks. This article studies the influence of residual connections on the hypothesis complexity of the neural network in terms of the covering number of its hypothesis space. We first present an upper bound of the covering number of networks with residual connections. This bound shares a similar structure with that of neural networks without residual connections. This result suggests that moving a weight matrix or nonlinear activation from the bone to a vine would not increase the hypothesis space. Afterward, an O(1 / √N) margin-based multiclass generalization bound is obtained for ResNet, as an exemplary case of any deep neural network with residual connections. Generalization guarantees for similar state-of-the-art neural network architectures, such as DenseNet and ResNeXt, are straightforward. According to the obtained generalization bound, we should introduce regularization terms to control the magnitude of the norms of weight matrices not to increase too much, in practice, to ensure a good generalization ability, which justifies the technique of weight decay.},
	number = {12},
	urldate = {2024-11-21},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {He, Fengxiang and Liu, Tongliang and Tao, Dacheng},
	month = dec,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Deep learning, Neural networks, Training, Correlation, Complexity theory, learning theory, Upper bound},
	pages = {5349--5362},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\WWHUR233\\8984747.html:text/html;Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\H6G2LSD5\\He et al. - 2020 - Why ResNet Works Residuals Generalize.pdf:application/pdf},
}

@inproceedings{liu_towards_2019,
	title = {Towards {Understanding} the {Importance} of {Shortcut} {Connections} in {Residual} {Networks}},
	volume = {32},
	abstract = {Residual Network (ResNet) is undoubtedly a milestone in deep learning. 
ResNet is equipped with shortcut connections between layers, and exhibits efficient training using simple first order algorithms. Despite of the great empirical success, the reason behind is far from being well understood. In this paper, we study a two-layer non-overlapping convolutional ResNet. Training such a network requires solving a non-convex optimization problem with a spurious local optimum. We show, however, that gradient descent combined with proper normalization, avoids being trapped by the spurious local optimum, and converges to a global optimum in polynomial time, when the weight of the first layer is initialized at 0, and that of the second layer is initialized arbitrarily in a ball. Numerical experiments are provided to support our theory.},
	urldate = {2024-11-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Tianyi and Chen, Minshuo and Zhou, Mo and Du, Simon S and Zhou, Enlu and Zhao, Tuo},
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\LIQHWIAR\\Liu et al. - 2019 - Towards Understanding the Importance of Shortcut C.pdf:application/pdf},
}

@inproceedings{huang_densely_2017,
	title = {Densely {Connected} {Convolutional} {Networks}},
	doi = {10.1109/CVPR.2017.243},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
	urldate = {2024-11-21},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
	month = jul,
	year = {2017},
	note = {ISSN: 1063-6919},
	keywords = {Neural networks, Training, Convolution, Convolutional codes, Network architecture, Road transportation},
	pages = {2261--2269},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\MQ9PE4X6\\8099726.html:text/html;Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\4FPPGGFE\\Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf:application/pdf},
}

@inproceedings{veit_residual_2016,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'16},
	title = {Residual networks behave like ensembles of relatively shallow networks},
	isbn = {978-1-5108-3881-9},
	abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
	urldate = {2024-11-21},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Veit, Andreas and Wilber, Michael and Belongie, Serge},
	month = dec,
	year = {2016},
	pages = {550--558},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\EUMAVEDR\\Veit et al. - 2016 - Residual networks behave like ensembles of relativ.pdf:application/pdf},
}

@inproceedings{wang_is_2020,
	address = {Yokohama, Japan},
	title = {Is the {Skip} {Connection} {Provable} to {Reform} the {Neural} {Network} {Loss} {Landscape}?},
	isbn = {978-0-9992411-6-5},
	doi = {10.24963/ijcai.2020/387},
	abstract = {The residual network is now one of the most effective structures in deep learning, which utilizes the skip connections to “guarantee” the performance will not get worse. However, the non-convexity of the neural network makes it unclear whether the skip connections do provably improve the learning ability since the nonlinearity may create many local minima. In some previous works [Freeman and Bruna, 2016], it is shown that despite the nonconvexity, the loss landscape of the two-layer ReLU network has good properties when the number m of hidden nodes is very large. In this paper, we follow this line to study the topology (sub-level sets) of the loss landscape of deep ReLU neural networks with a skip connection and theoretically prove that the skip connection network inherits the good properties of the two-layer network and skip connections can help to control the connectedness of the sub-level sets, such that any local minima worse than the global minima of some two-layer ReLU network will be very “shallow”. The “depth” of these local minima are at most O(m(η−1)/n), where n is the input dimension, η {\textless} 1. This provides a theoretical explanation for the effectiveness of the skip connection in deep learning.},
	language = {en},
	urldate = {2024-11-21},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Wang, Lifu and Shen, Bo and Zhao, Ning and Zhang, Zhiyuan},
	month = jul,
	year = {2020},
	pages = {2792--2798},
	file = {Wang et al. - 2020 - Is the Skip Connection Provable to Reform the Neur.pdf:C\:\\Users\\edwin\\Zotero\\storage\\9FBX2G9V\\Wang et al. - 2020 - Is the Skip Connection Provable to Reform the Neur.pdf:application/pdf},
}

@inproceedings{dong_attention_2021,
	title = {Attention is not all you need: pure attention loses rank doubly exponentially with depth},
	shorttitle = {Attention is not all you need},
	abstract = {Attention-based architectures have become ubiquitous in machine learning. Yet, our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms—or paths—each involving the operation of a sequence of attention heads across layers. Using this path decomposition, we prove that self-attention possesses a strong inductive bias towards "token uniformity". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the convergence results on standard transformer architectures.},
	language = {en},
	urldate = {2024-11-21},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {2793--2803},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\H88RM4SK\\Dong et al. - 2021 - Attention is not all you need pure attention lose.pdf:application/pdf},
}

@inproceedings{li_visualizing_2018,
	title = {Visualizing the {Loss} {Landscape} of {Neural} {Nets}},
	volume = {31},
	abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
	urldate = {2024-11-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\V9WDL42X\\Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets.pdf:application/pdf},
}
